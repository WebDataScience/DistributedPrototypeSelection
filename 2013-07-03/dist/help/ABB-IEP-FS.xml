<method>

	<name>Automatic Branch and Bound using Inconsistent Examples Pairs Measure</name>

	<reference>  

		<ref>H. Liu and L. Yu. "Toward Integrating Feature Selection Algorithms for Classification and Clustering", 
			IEEE Trans. onKnowledge and Data Engineering,17(4), 491-502, 2005.</ref>
		<ref>H. Liu y H. Motoda, Feature Selection for Knowledge Discovery and Data Mining, Kluwer Academic Publishers, 1998.</ref>

	</reference>

	<generalDescription>  

		<type>Non Evolutionary Filter method</type>

		<objective>Feature Selection. This method allows search to follow feature subsets</objective>

		<howWork>Automatic Branch and Bound algorithm starts with a full set of features, and removes one feature at a time. If 
			the full set contains three features, the root is (1 1 1) where ‘1’ means presence of the corresponding 
			feature and ‘0’ its absence. Its child nodes are (1 1 0), (1 0 1), and (0 1 1), etc. When there is 
			no restriction on expanding nodes in the search space, this could lead to an exhaustive 
			search. However, if each node is evaluated by a measure U and an upper limit is set for 
			the acceptable values of U , then Branch and Bound backtracks whenever an infeasible 
			node is discovered. If U is monotonic, no feasible node is omitted as a result of 
			early backtracking and, therefore, savings in search time do not sacrifice the 
			optimality of the selected subset, and hence it is a non-exhaustive yet complete 
			search. This algorithm uses the concept of approximate monotonicity which means 
			that branch and bound can continue even after encountering a node that does not satisfy 
			the bound. But the node that finally is accepted must satisfy the set bound.
		</howWork>

		<parameterSpec>  
			<param>paramKNN: is the number of nearest neighbours used by the k-NN classifier</param>
			
		</parameterSpec>

		<properties>

			<continuous>Yes</continuous>

			<discretized>Yes</discretized>

			<integer>Yes</integer>

			<nominal>Yes</nominal>

			<valueLess>No</valueLess>

			<impreciseValue>No</impreciseValue>

		</properties>

	</generalDescription>

	<example>

Problem type: Preprocess
Method: ABB-IEP-FS
Dataset: Car
Training set: car-10-1tra.dat
Test set: car-10-1tst.dat
Parameters: default values


After the execution of RunKeel.jar we can see the training and test datasets modified only with the selected features.

And the extra file with the classification error in test validation result0e0.txt:

result0e0.txt

RESULTS generated at Sun Jul 04 19:44:59 CEST 2010 
--------------------------------------------------
Algorithm Name: Automatic Branch and Bound (IEP)

PARTITION Filename: ../datasets/car/car-10-1tra.dat
---------------

Features selected: 
Buying - Maint - Doors - Persons - Lug_boot - Safety - 

6 features of 6

Error in test (using train for prediction): 0.2023121387283237
Error in test (using test for prediction): 0.19653179190751446
---------------



</example>

</method>