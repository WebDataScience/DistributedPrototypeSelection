<method>

	<name>Apriori-C</name>

	<reference>  

		<ref>V. Jovanoski, N. Lavrac. Classification Rule Learning with APRIORI-C. 10th Portuguese Conference on Artificial Intelligence on Progress in Artificial Intelligence, Knowledge Extraction, Multi-agent Systems, Logic Programming and Constraint Solving (EPIA 2001). Lecture Notes In Computer Science 2258, Porto (Portugal, 2001) 44-51.</ref>

	</reference>

	<generalDescription>  

		<type>Classification model by association rules.</type>

		<objective>The AprioriC is an upgrade of the Apriori algorithm to deal with classification problems.</objective>

		<howWork>We first create the 1-items that are items with only one pair variable-value (an attribute value or a class value). Then we check the support of all the itemsets created and delete the unsupported itemsets. Support may be defined as the number of examples which contain a item divided between the total number of examples. 
Then we start a loop in order to create the rules. We start creating the 2-items from the 1-items, and check their support deleting those which support is below the desired threshold. After that we select all items that have one target item at their right-hand side, that is, a value belonging to a class. We check their confidence (accuracy for the covered examples by the rule) and if it is above a selected threshold we create the rule and delete the itemset. We repite the loop while there are itemsets remaining, creating the 3-items from the 2-items, 4-items from 3-items and so on.
Finally we prune the number of rules, selecting the best rules globally or the best rules for each class.
</howWork>

		<parameterSpec>  

			<param>MinSupport = Is the threshold for the support value to create the items.</param>
			<param>MinConfidence = Is the threshold for the confidecen value to creat the rules.</param>
			<param>Number_of_Rules = Is the maximum number of rules permited. </param>
			<param>Postpruning_type = We can either select the maximum number of rules regardless its class or the maximum number of rules for each class indepently. </param>

		</parameterSpec>

		<properties>

			<continuous>No</continuous>

			<discretized>Yes</discretized>

			<integer>Yes</integer>

			<nominal>Yes</nominal>

			<valueLess>No</valueLess>

			<impreciseValue>No</impreciseValue>

		</properties>

	</generalDescription>

	<example>
	Problem type: Classification
	Method: Apriori-C
	Dataset: haberman
	Training set: haberman-10-1tra.dat
	Test set: haberman-10-1tst.dat
	Test Show results: StatChekCL
	Parameters: 
	MinSupport = 0.03
	MinConfidence = 0.9
	Number_of_Rules = 5
	Postpruning_type = SELECT_N_RULES_PER_CLASS

After the execution of RunKeel.jar we can see into the experiment\results\StatCheckCL folder the classification results for the test set:

TEST RESULTS
============
Classifier= 
Fold 0 : CORRECT=0.7096774193548387 N/C=0.0 
Global Classification Error + N/C:
0.2903225806451613
Correctly classified:
0.7096774193548387
Global N/C:
0.0 

We can also see the output and target classes for each case of the test set (result0.tst) in Experiment\Results\Clas-Apriori:

@relation  haberman
@attribute Age integer[30,83]
@attribute Year integer[58,69]
@attribute Positive integer[0,52]
@attribute class{positive,negative}
@inputs Age,Year,Positive
@outputs class
@data
negative negative
negative negative
negative negative
negative negative
negative negative
negative negative
negative negative
positive negative
negative negative
positive negative
negative negative
negative negative
negative negative
negative negative
negative negative
negative negative
negative negative
negative negative
positive negative
negative negative
negative negative
positive negative
positive negative
positive negative
negative negative
negative negative
negative negative
positive negative
positive negative
negative negative
positive negative


And the Rule Set and Statistics (result0e0.txt) in Experiment\Results\Clas-Apriori:


Rule 1: IF  Year =  61  AND Positive =  0  THEN class -&gt; negative  -- Support: 10

####Final Results####

Rule Size: 1
Avg. Significance (train): 6.07130237249568
Avg. Significance (test): 50.51457288616511
------------------
Accuracy Training: 0.7381818181818182
Accuracy Test: 0.7096774193548387

TIME (sec): 0
</example>

</method>