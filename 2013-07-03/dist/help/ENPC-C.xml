<method>

	<name>An Adaptive Michigan Approach PSO for Nearest Prototype Classification</name>

	<reference>  

		<ref> Evolutionary design of nearest prototype classifiers, Journal of Heuristics, vol. 10, no. 4, pp. 431--454, 2004. </ref>

	</reference>

	<generalDescription>  

		<type>Preprocess Method. Data Reduction. Instance Generation. Evolutionary algorithm.</type>

		<objective> Generation of optimal training set of prototypes </objective>

		<howWork>
            Usually, genetic algorithms are very dependent on some crucial parameters that have to be found by a trial and error process. One of the most important characteristics of this method is the lack of initial conditions, ENPC does not need any parameter.
Initially, this method selects a random prototype from the TR that it is introduced to GS. Next, while the end condition is achived five different operators are applied to GS. These operators focus their attention on defining regions in the search space.
First of all, ENPC obtains information about the search, i.e. it checks the quality of GS with regard to TR with different measures based on the NN.
Then, GS suffers a mutation phase where prototypes are re-labeled with the most populate class in each region of the search space. In order to refine GS a reproduction operator is introduced to generate new prototypes. Each prototype p from the GS has the opportunity of introducing a new prototype to increase its own quality with the objective of defining different regions. Next, a fight operator is able to refine the regions. This step does not modify the prototypes of GS it is only used to define again the different groups that we can find in GS. When the different regions have been established, ENPC executes the move operator. It consists in relocating each protoype in the best expected place, i.e. it is moved to the centroid of its region. Finally, a die operator is used to remove some prototypes from GS that are not relevant. 
            
				</howWork>

		<parameterSpec>  

            <param>MaxIter: The training phase ends after MaxIter iterations, default MaxIter  = 250  </param>

		</parameterSpec>

		<properties>

			<continuous>Yes</continuous>

			<discretized>Yes</discretized>

			<integer>Yes</integer>

			<nominal>Yes</nominal>

			<valueLess>No</valueLess>

			<impreciseValue>No</impreciseValue>

		</properties>

	</generalDescription>

	<example>Problem type: Generation
Method: IG-ENPC
Dataset: ecoli
Training set: ecoli-10-1tra.dat
Parameters: default values

We can see output set in Experiment\Results\IG-ENPC:

@relation  ecoli
@attribute mcg real[0.0,0.89]
@attribute gvh real[0.16,1.0]
@attribute lip real[0.48,1.0]
@attribute chg real[0.5,1.0]
@attribute aac real[0.0,0.88]
@attribute alm1 real[0.03,1.0]
@attribute alm2 real[0.0,0.99]
@attribute class{cp,im,pp,imU,om,omL,imL,imS}
@inputs mcg,gvh,lip,chg,aac,alm1,alm2
@outputs class
@data
0.24, 0.43, 0.48, 0.5, 0.37, 0.28, 0.38, cp
0.38, 0.3, 0.48, 0.5, 0.43, 0.29, 0.39, cp
0.32, 0.33, 0.48, 0.5, 0.6, 0.06, 0.2, cp
0.23, 0.58, 0.48, 0.5, 0.37, 0.53, 0.59, cp
0.29, 0.41, 0.48, 0.5, 0.48, 0.38, 0.46, cp
0.0, 0.38, 0.48, 0.5, 0.42, 0.48, 0.55, cp
0.52, 0.44, 0.48, 0.5, 0.37, 0.36, 0.42, cp
0.22, 0.36, 0.48, 0.5, 0.35, 0.39, 0.47, cp
0.25, 0.4, 0.48, 0.5, 0.46, 0.44, 0.52, cp
0.24, 0.57, 0.48, 0.5, 0.63, 0.34, 0.43, cp
0.61, 0.45, 0.48, 0.5, 0.48, 0.35, 0.41, cp
0.18, 0.3, 0.48, 0.5, 0.46, 0.24, 0.35, cp
0.45, 0.4, 0.48, 0.5, 0.61, 0.74, 0.77, im
0.0, 0.51, 0.48, 0.5, 0.35, 0.67, 0.44, im
0.11, 0.5, 0.48, 0.5, 0.58, 0.72, 0.68, im
0.58, 0.55, 0.48, 0.5, 0.57, 0.7, 0.74, im
0.31, 0.44, 0.48, 0.5, 0.5, 0.79, 0.82, im
0.52, 0.39, 0.48, 0.5, 0.65, 0.71, 0.73, im
0.61, 0.6, 0.48, 0.5, 0.44, 0.39, 0.38, pp
0.7, 0.64, 0.48, 0.5, 0.47, 0.51, 0.47, pp
0.69, 0.66, 0.48, 0.5, 0.41, 0.5, 0.25, pp
0.69, 0.59, 0.48, 0.5, 0.46, 0.44, 0.52, pp
0.79, 0.41, 0.48, 0.5, 0.66, 0.81, 0.83, imU
0.64, 0.45, 0.48, 0.5, 0.67, 0.61, 0.66, imU
0.57, 0.38, 0.48, 0.5, 0.06, 0.49, 0.33, imU
0.68, 0.76, 0.48, 0.5, 0.84, 0.45, 0.27, om
0.39, 0.31, 0.48, 0.5, 0.38, 0.34, 0.43, cp
0.33, 0.45, 0.48, 0.5, 0.45, 0.88, 0.89, im
0.66, 0.74, 0.48, 0.5, 0.31, 0.38, 0.43, pp
0.78, 0.68, 0.48, 0.5, 0.83, 0.4, 0.29, om


Results (one execution)):
Reduction =  90%
Training success = 82.45033112582782 %
Test success =
Time = 137.425 s.

</example>

</method>