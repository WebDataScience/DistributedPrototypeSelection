<method>

	<name>Ensemble Neural Network for Regression Problems</name>

	<reference>
			<ref>N. Garcia-Pedrajas, C. Garcia-Osorio and C. Fyfe. 
			Nonlinear Boosting Projections for Ensemble Construction. 
			Journal of Machine Learning Research 8 (2007) 1-33.
			</ref>
	</reference>


	<generalDescription>  

		<type> Regression model by a ensemble method.</type>

		<objective> 
		To perform a regression task by means of a combination of different classifiers, homogeneous or heterogeneous.
		</objective>

		<howWork>This methods employs an ensemble construction based on the use of nonlinear
		projections to achieve both accuracy and diversity of individual classifiers. Specificially it
		combines the philosophy of boosting, putting more effort on difficult instances, with the basis of
		the random subspace method. Instead of using a random subspace, this methods constructs a projection 
		taking into account the instances which have posed most difficulties to
		previous classifiers. In this way, consecutive nonlinear projections are created by a neural network
		trained using only incorrectly classified instances. The feature subspace induced by the hidden layer
		of this network is used as the input space to a new classifier.		
		</howWork>

		<parameterSpec>  
  			<param> Hidden_layers: number of hidden layers in the neural network</param>
  			<param> Hidden_nodes: number of nodes in each hidden layer of the neural network</param>
  			<param> Transfer: Transfer function. May be the Logarithmic function, the Hypertangent or a Lineal function</param>
  			<param> Eta: Learning coefficient</param>
  			<param> Alpha: Momentum coefficient</param>
  			<param> Lambda: </param>
  			<param> Cycles: Epochs in the training step</param>
  			<param> Improve: </param>
  			<param> Sampling: Sampling type. May have any of this types: Ada, Arcing (Adaptively resampling and combining), Bagging or None</param>
  			<param> Ensemble_method: </param>
  			<param> Combination: </param>
  			<param> Networks: Number of Neural networks used </param>
		</parameterSpec> 
		
		<properties>
			<continuous>	   Yes 	</continuous>
			<discretized>	   Yes	</discretized>
			<integer>		   Yes	</integer>
			<nominal>		   Yes	</nominal>
			<valueLess>		   Yes	</valueLess>
			<impreciseValue>   No	</impreciseValue>
		</properties>
	</generalDescription>

	<example>


		Problem type: Regression 
		Method: Regr-Ensemble
		Dataset: daily_electric_energy (10 fold cross validation run)
		Training set: daily_electric_energy-10-1tra.dat to daily_electric_energy-10-10tra.dat 
		Test set: daily_electric_energy-10-1tst.dat and daily_electric_energy-10-10tst.dat
		Test Show results: Vis-Regr-Check
		Parameters: default values
		
		After the execution of RunKeel.jar we can see into the results/Vis-Regr-Check/TSTRegr-Ensemble
		folder the regression results for the test set:

		TEST RESULTS
		============
		Model = 
		MSE of all folds:
		Fold 0 : 0.02275848390779774 
		Fold 1 : 0.03058750120717725 
		Fold 2 : 0.043390533346715034 
		Fold 3 : 0.025318230277020563 
		Fold 4 : 0.06914503872592918 
		Fold 5 : 0.03334446804612946 
		Fold 6 : 0.023867703713124126 
		Fold 7 : 0.0277206177992942 
		Fold 8 : 0.04323678221195007 
		Fold 9 : 0.03709154553314344 
		Global MSE:
		0.03564609047682811 
		Global stdev:
		0.013217599150426322 
		TRAIN RESULTS
		============
		Model = 
		MSE of all folds:
		Fold 0 : 0.032753689010687544 
		Fold 1 : 0.031710660437852166 
		Fold 2 : 0.03382809219301402 
		Fold 3 : 0.032588314385692645 
		Fold 4 : 0.031472904577830106 
		Fold 5 : 0.03202127866146735 
		Fold 6 : 0.0324436463530498 
		Fold 7 : 0.032043296507059525 
		Fold 8 : 0.03045118868409791 
		Fold 9 : 0.031434836651197985 
		Global MSE:
		0.03207479074619491 
		Global stdev:
		8.648160483686263E-4 
				
	</example>
		
</method>
