<method>

	<name>Fast Condensed Nearest Neighbor</name>

	<reference>  

		<ref>F. Angiulli. Fast nearest neighbor condensation for large data sets classification. IEEE Transactions on Knowledge and Data Engineering 19:11 (2007) 1450-1464.</ref>

	</reference>

	<generalDescription>  

		<type>Preprocess Method. Data Reduction. Instance Selection. Incremental.</type>

		<objective>Reduce the size of the training set without losing precision or accuracy in order to a posterior classification</objective>

		<howWork>The consistent subset S is initialized to the centroids of the classes contained in the training set T. Then, during each iteration of the algorithm, for each point p in S, a point q of T belonging to the Voronoi cell of p but having a different class label is selected and added to S. The algorithm stops when no further point can be added to S, that is, when T is correctly classified using S.</howWork>

		<parameterSpec>  

			<param>Number of neighbors: Integer value. Number of nearest instances considered to classify an example using the K-Nearest Neighbor Rule</param>

		</parameterSpec>

		<properties>

			<continuous>Yes</continuous>

			<discretized>Yes</discretized>

			<integer>Yes</integer>

			<nominal>Yes</nominal>

			<valueLess>No</valueLess>

			<impreciseValue>No</impreciseValue>

		</properties>

	</generalDescription>

	<example>Problem type: Classification 
Method: IS-FCNN
Dataset: iris
Training set: iris-10-1tra.dat
Parameters: default values

We can see output set in Experiment\Results\IS-FCNN:

@relation iris
@attribute sepalLength real [4.3, 7.9]
@attribute sepalWidth real [2.0, 4.4]
@attribute petalLength real [1.0, 6.9]
@attribute petalWidth real [0.1, 2.5]
@attribute class {Iris-setosa, Iris-versicolor, Iris-virginica}
@data
5.4,3.4,1.7,0.2,Iris-setosa
5.9,3.0,4.2,1.5,Iris-versicolor
6.1,2.8,4.0,1.3,Iris-versicolor
5.8,2.7,5.1,1.9,Iris-virginica
5.7,2.5,5.0,2.0,Iris-virginica
</example>

</method>