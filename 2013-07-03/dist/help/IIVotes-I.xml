<method>
	<name>
		IIVotes: SPIDER + IVotes with C4.5 Decision Tree as Base Classifier
	</name>
	<reference>  
		<ref>
			J. Blaszczynski, M. Deckert, J. Stefanowski, and S. Wilk. Integrating selective pre-processing of imbalanced data with ivotes ensemble, in Rough Sets and Current Trends in Computing, ser. LNSC, M. Szczuka, M. Kryszkiewicz, S. Ramanna, R. Jensen, and Q. Hu, Eds. Springer Berlin / Heidelberg, 2010, vol. 6086, pp. 148-157.
		</ref>
		<ref>
			L. Breiman. Pasting small votes for classification in large databases and  on-line. Machine Learning, vol. 36, pp. 85--103, 1999.
		</ref>
	</reference>
	<generalDescription>  
		<type>
			Ensemble of classifiers. Bagging. Importance sampling.
		</type>
		<objective>
			To determine a set of decision trees that on the basis of answers to questions about the input attributes predicts correctly the value of the target attribute.
		</objective>
		<howWork>
			IIVotes, Imbalanced IVotes  integrates SPIDER data preprocessing technique with IVotes (a preprocessing phase is applied in each iteration before training the new classifier). This method has the advantage of not needing to define the number of bags, since the algorithm stops when the out-of-bag error estimation no longer decreases. IVotes is a variation of Bagging where the sampling is done taking into account the importance of each instance.
        </howWork>
		<parameterSpec>  
			<param>
				prune: wether to prune or not prune the tree. It is a boolean value that determines if the algorithm applies a prune process after building the tree
			</param>
			<param>
				confidence: is the confidence level. It is a float value that determines what is the minimal confidence that must has a leaf in order to can be considered in the tree
			</param>
			<param>
				minItemsets: is the minimum number of item-sets per leaf. It is an integer value that determines how much data instances must contain a leaf in order to can be created
			</param>
			<param>
				Number of Classifiers: is the maximum number of iterations that the boosting procedure will be executed
			</param>
			<param>
				Train Method: is the method used to train the classifier in each iteration (using resampling or cost-sensitive classifier)
			</param>
			<param>
				Preprocess Type: is the type of SPIDER preprocessing  that will be used in each iteration (WEAK, RELABEL or STRONG)
			</param>
		</parameterSpec>
		<properties>
			<continuous>Yes</continuous>
			<discretized>Yes</discretized>
			<integer>Yes</integer>
			<nominal>Yes</nominal>
			<valueLess>Yes</valueLess>
			<impreciseValue>No</impreciseValue>
		</properties>
	</generalDescription>
<example>
Problem type: Imbalanced
Method: Cost Sensitive Boosting with C4.5 Decision Tree as Base Classifier
Dataset: abalone9-18
Training set: abalone9-18-5-5-1tra.dat
Test set: abalone9-18-5-5-1tst.dat
Test Show results: TSTImb-IIVotes
Parameters: default values

After the execution of RunKeel.jar we can see the classification results for the test set:

G-mean in Training: 0.9829431308484223
F-mean in Training: 0.955223880597015
TPrate in Training: 0.9696969696969697
G-mean in Test: 0.5731513024140634
F-mean in Test: 0.42857142857142855
TPrate in Test: 0.3333333333333333
Accuracy in training: 0.9948630136986302
Accuracy in test: 0.9455782312925171

We can also see the output and target classes for each case of the test set (result0.tst) in Experiment\Results\Imb-AdaC2.abalone9-18:

@relation  abalone9-18
@attribute Sex{M,F,I}
@attribute Length real[0.075,0.815]
@attribute Diameter real[0.055,0.65]
@attribute Height real[0.0,1.13]
@attribute Whole_weight real[0.0020,2.8255]
@attribute Shucked_weight real[0.0010,1.488]
@attribute Viscera_weight real[5.0E-4,0.76]
@attribute Shell_weight real[0.0015,1.005]
@attribute Rings{positive,negative}
@inputs Sex,Length,Diameter,Height,Whole_weight,Shucked_weight,Viscera_weight,Shell_weight
@outputs Rings
@data
negative negative
negative negative
negative negative
negative negative
negative negative
positive negative
negative negative
negative negative
negative negative
negative negative
positive positive
negative negative
negative negative
positive negative
positive positive
positive negative
negative negative
negative negative
negative negative
negative negative
negative negative
negative negative
negative negative
negative negative
negative negative
negative negative
negative negative
negative negative
negative negative
negative negative
negative negative
...
</example>

</method>
