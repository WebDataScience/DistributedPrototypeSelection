<method>

	<name>Population-Based Incremental Learning</name>

	<reference>  

		<ref>J.R. Cano, F. Herrera, M. Lozano. Using evolutionary algorithms as instance selection for data reduction in KDD: An experimental study. IEEE Transaction on Evolutionary Computation 7:6 (2003) 561-575</ref>

	</reference>

	<generalDescription>  

		<type>Preprocess Method. Data Reduction. Instance Selection. Evolutionary</type>

		<objective>Reduce the size of the training set without losing precision or accuracy in order to a posterior classification</objective>

		<howWork>Application of a Population Based on Incremental Learning Estimation of Distribution Algorithm to the Instance Selection Problem. PBIL is a specific EA designed for binary search spaces. The PBIL algorithm attempts to
		explicitly maintains statistics about the search space to decide where to sample
		next. The object of the algorithm is to create a real valued probability vector Vp,
		which, when sampled, reveals high quality solution vectors with high probability.
		Initially, the values of Vp are set at 0.5. Sampling from this vector yields random
		solution vectors because the probability of generating a 1 or 0 is equal. As the
		search progresses, the values of Vp gradually shift to represent high evaluation
		solution vectors through the following process.</howWork>

		<parameterSpec>
			<param>Number of Evaluations: Integer value. Maximal number of evaluations allowed in the run of the algorithm.</param>
			<param>Population Size: Integer value. Number of chromosome belonging to the population.</param>
			<param>LR: Real value. Learning Rate.</param>
			<param>Mut_shift: Real value. Probability for changing the random direction of shift.</param>
			<param>Mutation Probability: Real value. Probability associated to the mutation operator</param>
			<param>Negative LR: Real value. Negative Learning Rate</param>
			<param>Alfa Equilibrate Factor: Real value. Trade-off factor that controls the balaning between reduction and accuracy</param>
			<param>Initial value of Pv: Real value. Initial value in each position of the Vp vector.</param>
			<param>Number of neighbors: Integer value. Number of nearest instances considered to classify an example using the K-Nearest Neighbor Rule</param>
			<param>Distance Function: K-NN implements two distance functions. a) Euclidean with normalized attributed and b) HVDM (see paper D.R. Wilson, T.R. Martinez. Reduction Tecniques For Instance-Based Learning Algorithms. Machine Learning 38:3 (2000) 257-286.)</param>			</parameterSpec>

		<properties>

			<continuous>Yes</continuous>

			<discretized>Yes</discretized>

			<integer>Yes</integer>

			<nominal>Yes</nominal>

			<valueLess>No</valueLess>

			<impreciseValue>No</impreciseValue>

		</properties>

	</generalDescription>

	<example>Problem type: Classification 
Method: IS-PBIL
Dataset: iris
Training set: iris-10-1tra.dat
Parameters: default values

We can see output set in Experiment\Results\IS-PBIL:

@relation iris
@attribute sepalLength real [4.3, 7.9]
@attribute sepalWidth real [2.0, 4.4]
@attribute petalLength real [1.0, 6.9]
@attribute petalWidth real [0.1, 2.5]
@attribute class {Iris-setosa, Iris-versicolor, Iris-virginica}
@data
4.8,3.4,1.6,0.2,Iris-setosa
6.4,2.9,4.3,1.3,Iris-versicolor
5.6,2.7,4.2,1.3,Iris-versicolor
6.7,2.5,5.8,1.8,Iris-virginica
5.7,2.5,5.0,2.0,Iris-virginica
</example>

</method>